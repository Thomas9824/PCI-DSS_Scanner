#!/usr/bin/env python3
"""
PCI Auto Scraper - Script combin√© pour d√©tecter les changements et t√©l√©charger automatiquement
Combine la d√©tection de changements et le t√©l√©chargement automatique des documents PCI DSS/SAQ
"""

import os
import sys
import time
import logging
import base64
from datetime import datetime
from typing import Dict, List, Optional
import shutil
import pandas as pd
import resend

# chemins des sous-projets
script_dir = os.path.dirname(os.path.abspath(__file__))
pci_change_scraper_path = os.path.join(script_dir, 'pci_change_scraper')
pci_pdf_scraper_path = os.path.join(script_dir, 'pci_pdf_scraper')
pci_pdf_extractor_path = os.path.join(script_dir, 'pci_pdf_extractor')

sys.path.insert(0, pci_change_scraper_path)
sys.path.insert(0, pci_pdf_scraper_path)
sys.path.insert(0, pci_pdf_extractor_path)

# Import des modules des sous-projets
try:
    from pci_scraper import PCIDocumentScraper
    from pci_pdf_scraper import PCIScraperEnhanced
    from testv5 import PCIRequirementsExtractor as PCIRequirementsExtractorFR
    from testv5_EN import PCIRequirementsExtractor as PCIRequirementsExtractorEN
    from testv5_ES import PCIRequirementsExtractor as PCIRequirementsExtractorES
    from testv5_DE import PCIRequirementsExtractor as PCIRequirementsExtractorDE
    from testv5_PT import PCIRequirementsExtractor as PCIRequirementsExtractorPT
except ImportError as e:
    print(f" Erreur d'import des modules: {e}")
    print("   pip install -r requirements.txt")
    sys.exit(1)

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration de Resend pour l'envoi d'emails
import os
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

resend.api_key = os.getenv('RESEND_API_KEY')
if not resend.api_key:
    logger.error("RESEND_API_KEY non trouv√©e dans les variables d'environnement. Cr√©ez un fichier .env avec votre cl√© API.")
    sys.exit(1)

class PCIAutoScraper:
    """
    Scraper automatique qui combine la d√©tection de changements et le t√©l√©chargement
    """
    
    def __init__(self, headless: bool = True, download_dir: str = "downloads"):
        """
        Initialise le scraper automatique
        
        Args:
            headless: Mode headless pour les navigateurs
            download_dir: R√©pertoire de t√©l√©chargement des PDFs
        """
        self.headless = headless
        self.download_dir = download_dir
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Initialise les scrapers
        self.change_detector = None
        self.pdf_downloader = None
        
        # Statistiques
        self.stats = {
            'documents_checked': 0,
            'changes_detected': 0,
            'downloads_attempted': 0,
            'downloads_successful': 0,
            'new_documents': 0,
            'updated_versions': 0,
            'removed_documents': 0,
            'extracted_files': 0
        }
        
        # Stocker les fichiers extraits pour l'email
        self.extracted_csv_files = []
        
    def setup_scrapers(self):
        """Configure les scrapers pour la d√©tection et le t√©l√©chargement"""
        try:
            logger.info("Configuration des scrapers...")
            
            # Configure le d√©tecteur de changements avec chemins relatifs
            self.change_detector = PCIDocumentScraper(headless=self.headless)
            
            # Patch les m√©thodes pour utiliser le r√©pertoire de travail actuel
            script_dir = os.path.dirname(os.path.abspath(__file__))
            self.data_dir = script_dir  # Utilise le r√©pertoire du script pour les donn√©es
            
            # Override les m√©thodes qui utilisent des chemins cod√©s en dur
            original_load = self.change_detector.load_previous_data
            original_save = self.change_detector.save_to_csv
            original_save_report = self.change_detector.save_changes_report
            
            def patched_load_previous_data(filename="pci_documents.csv"):
                try:
                    csv_path = os.path.join(self.data_dir, filename)
                    if os.path.exists(csv_path):
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        logger.info(f"Donn√©es pr√©c√©dentes charg√©es depuis: {csv_path} ({len(df)} documents)")
                        return df
                    else:
                        logger.info("Aucun fichier de donn√©es pr√©c√©dentes trouv√©")
                        return None
                except Exception as e:
                    logger.error(f"Erreur lors du chargement des donn√©es pr√©c√©dentes: {e}")
                    return None
            
            def patched_save_to_csv(filename="pci_documents.csv", backup_previous=True):
                try:
                    if not self.change_detector.documents:
                        logger.warning("Aucun document √† sauvegarder")
                        return
                    
                    csv_path = os.path.join(self.data_dir, filename)
                    
                    # Sauvegarde l'ancien fichier si demand√©
                    if backup_previous and os.path.exists(csv_path):
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        backup_filename = f"pci_documents_backup_{timestamp}.csv"
                        backup_path = os.path.join(self.data_dir, backup_filename)
                        
                        shutil.copy2(csv_path, backup_path)
                        logger.info(f"Ancienne version sauvegard√©e dans: {backup_filename}")
                    
                    # Cr√©e un DataFrame pandas
                    df = pd.DataFrame(self.change_detector.documents)
                    
                    # Ajoute un timestamp
                    df['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    # Sauvegarde en CSV
                    df.to_csv(csv_path, index=False, encoding='utf-8')
                    
                    logger.info(f"Documents sauvegard√©s dans: {csv_path}")
                    logger.info(f"Nombre de documents sauvegard√©s: {len(self.change_detector.documents)}")
                    
                except Exception as e:
                    logger.error(f"Erreur lors de la sauvegarde: {e}")
            
            def patched_save_changes_report(changes, timestamp=None):
                # Ne fait rien - pas de cr√©ation de fichier rapport
                pass
            
            # Applique les patches
            self.change_detector.load_previous_data = patched_load_previous_data
            self.change_detector.save_to_csv = patched_save_to_csv
            self.change_detector.save_changes_report = patched_save_changes_report
            
            # Configure le t√©l√©chargeur PDF avec le bon r√©pertoire
            full_download_path = os.path.abspath(self.download_dir)
            self.pdf_downloader = PCIScraperEnhanced(download_dir=full_download_path)
            
            return True
            
        except Exception as e:
            logger.error(f" Erreur lors de la configuration des scrapers: {e}")
            return False
    
    def detect_changes(self) -> Optional[Dict[str, List]]:
        """
        D√©tecte les changements dans les documents PCI DSS/SAQ
        
        Returns:
            Dictionnaire des changements d√©tect√©s ou None en cas d'erreur
        """
        try:
            logger.info(" D√©marrage de la d√©tection de changements...")
            
            # Charge les donn√©es pr√©c√©dentes
            previous_data = self.change_detector.load_previous_data("pci_documents.csv")
            
            # Configure le driver
            self.change_detector.setup_driver()
            
            # Scrape tous les documents actuels
            current_documents = self.change_detector.scrape_all_documents()
            self.stats['documents_checked'] = len(current_documents)
            
            if not current_documents:
                logger.error("Aucun document trouv√© lors du scraping")
                return None
            
            # Compare avec les donn√©es pr√©c√©dentes
            changes = self.change_detector.compare_versions(previous_data)
            
            # Met √† jour les statistiques
            self.stats['new_documents'] = len(changes['new_documents'])
            self.stats['updated_versions'] = len(changes['updated_versions'])
            self.stats['removed_documents'] = len(changes['removed_documents'])
            self.stats['changes_detected'] = (
                self.stats['new_documents'] + 
                self.stats['updated_versions'] + 
                self.stats['removed_documents']
            )
            
            # Sauvegarde les nouvelles donn√©es
            self.change_detector.save_to_csv("pci_documents.csv", backup_previous=True)
            
            # Log des changements sans cr√©er de fichier rapport
            if self.stats['changes_detected'] > 0:
                logger.info(f"{self.stats['changes_detected']} changements d√©tect√©s !")
                for doc in changes['new_documents']:
                    logger.info(f"Nouveau: {doc['name']} ({doc['category']})")
                for change in changes['updated_versions']:
                    logger.info(f"Mis √† jour: {change['name']} ({change['category']}) - {change['old_version']} ‚Üí {change['new_version']}")
            else:
                logger.info("Aucun changement d√©tect√©")
            
            return changes
            
        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection de changements: {e}")
            return None
        finally:
            if self.change_detector and self.change_detector.driver:
                self.change_detector.close()
    
    def should_download(self, changes: Dict[str, List]) -> bool:
        """
        D√©termine s'il faut lancer un t√©l√©chargement bas√© sur les changements d√©tect√©s
        
        Args:
            changes: Dictionnaire des changements d√©tect√©s
            
        Returns:
            True s'il faut t√©l√©charger, False sinon
        """
        total_changes = (
            len(changes['new_documents']) + 
            len(changes['updated_versions'])
        )
        
        if total_changes > 0:
            logger.info(f"{total_changes} changements n√©cessitent un t√©l√©chargement")
            return True
        else:
            logger.info("Aucun t√©l√©chargement n√©cessaire")
            return False
    
    def download_changed_documents(self, changes: Dict[str, List]) -> bool:
        """
        T√©l√©charge uniquement les documents qui ont chang√©
        
        Args:
            changes: Dictionnaire des changements d√©tect√©s
            
        Returns:
            True si le t√©l√©chargement s'est bien pass√©, False sinon
        """
        try:
            logger.info("üì• D√©marrage du t√©l√©chargement s√©lectif des documents modifi√©s...")
            
            # Collecte les documents √† t√©l√©charger
            documents_to_download = []
            
            # Ajoute les nouveaux documents
            for doc in changes['new_documents']:
                documents_to_download.append(doc)
                logger.info(f"√Ä t√©l√©charger (nouveau): {doc['name']} ({doc['category']})")
            
            # Ajoute les documents avec versions mises √† jour
            for change in changes['updated_versions']:
                doc_info = {
                    'name': change['name'],
                    'category': change['category'],
                    'version': change['new_version']
                }
                documents_to_download.append(doc_info)
                logger.info(f"√Ä t√©l√©charger (mis √† jour): {change['name']} ({change['category']}) - {change['old_version']} ‚Üí {change['new_version']}")
            
            if not documents_to_download:
                logger.info("Aucun document √† t√©l√©charger")
                return True
            
            logger.info(f"{len(documents_to_download)} documents √† t√©l√©charger")
            
            # Cr√©e un dossier sp√©cifique pour cette session de t√©l√©chargement
            session_dir = os.path.join(self.download_dir, f"session_{self.timestamp}")
            os.makedirs(session_dir, exist_ok=True)
            
            # T√©l√©charge s√©lectivement les documents
            success = self.download_specific_documents(documents_to_download, session_dir)
            
            if success:
                # Copie les nouveaux fichiers dans le r√©pertoire principal
                main_download_dir = os.path.join(self.download_dir, "latest")
                os.makedirs(main_download_dir, exist_ok=True)
                
                downloaded_files = [f for f in os.listdir(session_dir) if f.endswith('.pdf')]
                self.stats['downloads_successful'] = len(downloaded_files)
                
                for file in downloaded_files:
                    src = os.path.join(session_dir, file)
                    dst = os.path.join(main_download_dir, file)
                    shutil.copy2(src, dst)
                    logger.info(f"üìÑ Copi√©: {file}")
                
                # Extraction automatique des PDFs t√©l√©charg√©s
                logger.info("üîç D√©marrage de l'extraction des exigences PCI DSS...")
                self.extract_downloaded_pdfs(downloaded_files, session_dir)
                
                logger.info(f"{len(downloaded_files)} documents t√©l√©charg√©s avec succ√®s")
                return True
            else:
                logger.error("√âchec du t√©l√©chargement s√©lectif")
                return False
                
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement s√©lectif: {e}")
            return False
    
    def download_specific_documents(self, documents_to_download: List[Dict], download_dir: str) -> bool:
        """
        T√©l√©charge des documents sp√©cifiques en utilisant le scraper PDF modifi√©
        
        Args:
            documents_to_download: Liste des documents √† t√©l√©charger
            download_dir: R√©pertoire de t√©l√©chargement
            
        Returns:
            True si le t√©l√©chargement r√©ussit
        """
        try:
            logger.info(f"üîß Configuration du t√©l√©chargeur pour {len(documents_to_download)} documents sp√©cifiques")
            
            # Configure le t√©l√©chargeur PDF pour le t√©l√©chargement s√©lectif
            selective_downloader = PCIScraperEnhanced(download_dir=download_dir)
            
            # Override la m√©thode pour filtrer seulement les documents chang√©s
            original_get_all_pdf_links = selective_downloader.get_all_pdf_links
            
            def selective_get_pdf_links():
                """Version modifi√©e qui filtre selon les documents √† t√©l√©charger avec matching pr√©cis"""
                logger.info("Recherche des liens PDF pour les documents sp√©cifiques...")
                
                # Obtient tous les liens PDF disponibles avec informations d√©taill√©es
                all_links = original_get_all_pdf_links()
                
                # Filtre pour ne garder que les documents qui ont chang√©
                filtered_links = []
                
                for link_info in all_links:
                    # V√©rifie si ce lien correspond √† un document √† t√©l√©charger
                    for doc in documents_to_download:
                        if self.matches_document_precise(link_info, doc):
                            filtered_links.append(link_info)
                            logger.info(f"Lien trouv√© pour: {doc['name']} (v{doc.get('version', 'N/A')}) -> {link_info['url']}")
                            break
                
                logger.info(f"üìä {len(filtered_links)} liens PDF filtr√©s sur {len(all_links)} disponibles")
                return filtered_links
            
            # Applique le filtre
            selective_downloader.get_all_pdf_links = selective_get_pdf_links
            
            # Lance le t√©l√©chargement s√©lectif
            selective_downloader.run()
            
            # V√©rifie les r√©sultats
            downloaded_files = [f for f in os.listdir(download_dir) if f.endswith('.pdf')] if os.path.exists(download_dir) else []
            
            if downloaded_files:
                logger.info(f"T√©l√©chargement s√©lectif r√©ussi: {len(downloaded_files)} fichiers")
                return True
            else:
                logger.warning("Aucun fichier t√©l√©charg√© lors du t√©l√©chargement s√©lectif")
                # Fallback: essaie de t√©l√©charger au moins quelques documents critiques
                return self.fallback_download(documents_to_download, download_dir)
                
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement sp√©cifique: {e}")
            return False
    
    def matches_document_precise(self, link_info: Dict, target_doc: Dict) -> bool:
        """
        M√©thode de matching pr√©cise utilisant les informations d√©taill√©es du document
        
        Args:
            link_info: Informations d√©taill√©es du lien (avec document_name, version, etc.)
            target_doc: Document cible √† matcher
            
        Returns:
            True si le document correspond exactement
        """
        try:
            # Extrait les informations du lien
            link_doc_name = link_info.get('document_name', '').lower().strip()
            link_version = link_info.get('version', '').lower().strip()
            link_category = link_info.get('category', '').lower().strip()
            
            # Extrait les informations du document cible
            target_name = target_doc.get('name', '').lower().strip()
            target_version = target_doc.get('version', '').lower().strip()
            target_category = target_doc.get('category', '').lower().strip()
            
            # Matching exact du nom et de la cat√©gorie
            name_match = link_doc_name == target_name
            category_match = any(cat in link_category for cat in [target_category, target_category.replace(' ', '')])
            
            # Si on a les versions, on les compare aussi
            version_match = True
            if target_version and target_version != 'n/a' and link_version and link_version != 'n/a':
                # Normalise les versions pour la comparaison
                target_version_clean = self.normalize_version(target_version)
                link_version_clean = self.normalize_version(link_version)
                version_match = target_version_clean == link_version_clean
            
            match_result = name_match and category_match and version_match
            
            if match_result:
                logger.info(f"Match pr√©cis trouv√©: '{link_doc_name}' v{link_version} == '{target_name}' v{target_version}")
            else:
                logger.debug(f"Pas de match: '{link_doc_name}' v{link_version} != '{target_name}' v{target_version}")
                logger.debug(f"Name match: {name_match}, Category match: {category_match}, Version match: {version_match}")
            
            return match_result
            
        except Exception as e:
            logger.warning(f"Erreur lors du matching pr√©cis: {e}")
            return False
    
    def normalize_version(self, version: str) -> str:
        """
        Normalise une version pour la comparaison
        
        Args:
            version: Version √† normaliser
            
        Returns:
            Version normalis√©e
        """
        if not version:
            return ""
        
        # Supprime les espaces et convertit en minuscules
        normalized = version.lower().strip()
        
        # Supprime les caract√®res non essentiels
        import re
        normalized = re.sub(r'[^\w\.]', '', normalized)
        
        return normalized

    def extract_downloaded_pdfs(self, downloaded_files: List[str], session_dir: str):
        """
        Extrait automatiquement les exigences des PDFs t√©l√©charg√©s (EN et FR)
        
        Args:
            downloaded_files: Liste des fichiers PDF t√©l√©charg√©s
            session_dir: R√©pertoire contenant les PDFs
        """
        try:
            for pdf_file in downloaded_files:
                pdf_path = os.path.join(session_dir, pdf_file)
                
                # D√©termine la langue du document bas√© sur le nom du fichier
                pdf_name_without_ext = os.path.splitext(pdf_file)[0]
                language = self.detect_document_language(pdf_file)
                
                logger.info(f"üîç Analyse de langue pour {pdf_file}: {language}")
                
                # S√©lectionne l'extracteur appropri√© selon la langue
                if language == 'FR':
                    logger.info(f"üìã Extraction FR: {pdf_file}")
                    extractor = PCIRequirementsExtractorFR(pdf_path)
                elif language == 'ES':
                    logger.info(f"üìã Extraction ES: {pdf_file}")
                    extractor = PCIRequirementsExtractorES(pdf_path)
                elif language == 'DE':
                    logger.info(f"üìã Extraction DE: {pdf_file}")
                    extractor = PCIRequirementsExtractorDE(pdf_path)
                elif language == 'PT':
                    logger.info(f"üìã Extraction PT: {pdf_file}")
                    extractor = PCIRequirementsExtractorPT(pdf_path)
                else:  # EN par d√©faut
                    logger.info(f"üìã Extraction EN: {pdf_file}")
                    extractor = PCIRequirementsExtractorEN(pdf_path)
                
                output_file = os.path.join(session_dir, f"{pdf_name_without_ext}.csv")
                
                # Extrait les exigences
                requirements = extractor.extract_all_requirements()
                
                if requirements:
                    extractor.save_to_csv(output_file)
                    
                    # Ajoute le fichier CSV √† la liste pour l'email
                    self.extracted_csv_files.append(output_file)
                    self.stats['extracted_files'] += 1
                    
                    language = self.detect_document_language(pdf_file)
                    logger.info(f"‚úÖ Extraction {language} r√©ussie: {len(requirements)} exigences ‚Üí {os.path.basename(output_file)}")
                else:
                    logger.warning(f"‚ö†Ô∏è Aucune exigence extraite de {pdf_file}")
                    
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction des PDFs: {e}")
    
    def detect_document_language(self, filename: str) -> str:
        """
        D√©termine la langue d'un document bas√© sur son nom
        
        Args:
            filename: Nom du fichier PDF ou CSV
            
        Returns:
            Code de langue ('EN', 'FR', 'ES', 'DE', 'PT')
        """
        filename_lower = filename.lower()
        
        # Indicateurs de langue par priorit√©
        language_indicators = {
            'EN': ['_en.pdf', '_en.csv', '-en.pdf', '-en.csv', '_en_', '-en_', 'english'],
            'FR': ['_fr.pdf', '_fr.csv', '-fr.pdf', '-fr.csv', '_fr_', '-fr_', 'french', 'francais', 'merchant-fr', '-merchant-fr'],
            'ES': ['_es.pdf', '_es.csv', '-es.pdf', '-es.csv', '_es_', '-es_', 'spanish', 'espanol', 'merchant-es', '-merchant-es'],
            'DE': ['_de.pdf', '_de.csv', '-de.pdf', '-de.csv', '_de_', '-de_', 'german', 'deutsch', 'merchant-de', '-merchant-de'],
            'PT': ['_pt.pdf', '_pt.csv', '-pt.pdf', '-pt.csv', '_pt_', '-pt_', 'portuguese', 'portugues', 'merchant-pt', '-merchant-pt']
        }
        
        # V√©rification de chaque langue
        for lang_code, indicators in language_indicators.items():
            if any(indicator in filename_lower for indicator in indicators):
                return lang_code
        
        # Par d√©faut, consid√®re comme anglais si pas d'indicateur clair
        return 'EN'

    def matches_document(self, url: str, doc_name: str, doc_category: str) -> bool:
        """
        V√©rifie si une URL correspond EXACTEMENT √† un document recherch√©
        
        Args:
            url: URL du document
            doc_name: Nom du document recherch√©
            doc_category: Cat√©gorie du document
            
        Returns:
            True si l'URL correspond exactement au document
        """
        url_lower = url.lower()
        doc_name_lower = doc_name.lower()
        
        # Mapping pr√©cis des noms de documents vers leurs URLs sp√©cifiques
        exact_mappings = {
            # Documents PCI DSS principaux
            'pci dss': ['pci-dss-v4_0_1.pdf', 'pci-dss-v4-0-1.pdf'],
            'pci dss summary of changes': ['pci-dss-v4-0-to-v4-0-1-summary-of-changes', 'summary-of-changes'],
            'prioritized approach for pci dss': ['prioritized-approach-for-pci-dss'],
            'prioritized approach tool': ['prioritized-approach'],
            'pci dss quick reference guide': ['pci-dss-v4_x-qrg', 'quick-reference'],
            'pci dss v4.0 at a glance': ['pci-dss-v4-0-at-a-glance'],
            'asv resource guide': ['pci%20ssc%20asv%20resource%20guide', 'asv-resource'],
            'guidance for pci dss requirements': ['guidance-for-pci-dss-requirements'],
            'tra guidance': ['pci_dss_v4.x_tra_guidance', 'tra-guidance'],
            'vulnerability management': ['vulnerability%20management%20infographic'],
            'roc template': ['pci-dss-v4-0-1-roc-template'],
            'roc template summary of changes': ['roc-template-summary-of-changes'],
            
            # Documents SAQ sp√©cifiques
            'saq instructions and guidelines': ['saq-instructions-guidelines-pci-dss'],
            'saq a': ['pci-dss-v4_0_1-saq-a-r1.pdf', 'pci-dss-v4-0-1-saq-a.pdf'],
            'saq a-ep': ['pci-dss-v4-0-1-saq-a-ep.pdf'],
            'saq b': ['pci-dss-v4-0-1-saq-b.pdf'],
            'saq b-ip': ['pci-dss-v4-0-1-saq-b-ip.pdf'],
            'saq c': ['pci-dss-v4-0-1-saq-c.pdf'],
            'saq c-vt': ['pci-dss-v4-0-1-saq-c-vt.pdf'],
            'saq d merchant': ['pci-dss-v4-0-1-saq-d-merchant.pdf'],
            'saq d service provider': ['pci-dss-v4-0-1-saq-d-service-provider'],
            'saq p2pe': ['pci-dss-v4-0-1-saq-p2pe.pdf'],
            'saq spoc': ['pci-dss-v4-0-1-saq-spoc.pdf'],
            
            # AOC documents
            'aoc saq a': ['pci-dss-v4-0-1-aoc-for-saq-a'],
            'aoc saq a-ep': ['pci-dss-v4-0-1-aoc-for-saq-a-ep'],
            'aoc saq b': ['pci-dss-v4-0-1-aoc-for-saq-b.pdf'],
            'aoc saq b-ip': ['pci-dss-v4-0-1-aoc-for-saq-b-ip'],
            'aoc saq c': ['pci-dss-v4-0-1-aoc-for-saq-c.pdf'],
            'aoc saq c-vt': ['pci-dss-v4-0-1-aoc-for-saq-c-vt'],
            'aoc saq d - merchants': ['pci-dss-v4-0-1-aoc-for-saq-d-merchant'],
            'aoc saq d - service providers': ['pci-dss-v4-0-1-aoc-for-saq-d-service-provider'],
            'aoc saq p2pe': ['pci-dss-v4-0-1-aoc-for-saq-p2pe'],
            'aoc saq spoc': ['pci-dss-v4-0-1-aoc-for-saq-spoc'],
        }
        
        # Recherche exacte d'abord
        for doc_pattern, url_patterns in exact_mappings.items():
            if doc_pattern in doc_name_lower:
                for pattern in url_patterns:
                    if pattern.lower() in url_lower:
                        return True
        
        # Si pas de match exact, utilise une logique plus stricte
        # Seulement pour les cas o√π le nom du document correspond tr√®s pr√©cis√©ment
        
        # Pour SAQ Instructions and Guidelines - cas sp√©cial car il guide tous les SAQ
        if 'saq instructions and guidelines' in doc_name_lower:
            return 'saq-instructions-guidelines' in url_lower
        
        # Pour les documents PCI DSS principaux
        if doc_name_lower == 'pci dss':
            return 'pci-dss-v4_0_1.pdf' in url_lower or 'pci-dss-v4-0-1.pdf' in url_lower
        
        # Pour les SAQ sp√©cifiques - match tr√®s pr√©cis
        saq_types = ['saq a', 'saq b', 'saq c', 'saq d', 'saq p2pe', 'saq spoc']
        for saq_type in saq_types:
            if doc_name_lower.startswith(saq_type) and saq_type.replace(' ', '-') in url_lower:
                # V√©rification suppl√©mentaire pour √©viter les faux positifs
                if saq_type == 'saq a' and ('saq-a-ep' in url_lower or 'saq-a-r1' in url_lower):
                    return 'ep' in doc_name_lower or 'r1' in url_lower
                return True
        
        return False
    
    def fallback_download(self, documents_to_download: List[Dict], download_dir: str) -> bool:
        """
        T√©l√©chargement de fallback si le t√©l√©chargement s√©lectif √©choue
        
        Args:
            documents_to_download: Documents √† t√©l√©charger
            download_dir: R√©pertoire de t√©l√©chargement
            
        Returns:
            True si au moins quelques documents sont t√©l√©charg√©s
        """
        
        try:
            # Essaie de t√©l√©charger les documents les plus critiques
            critical_docs = [doc for doc in documents_to_download if 
                           'pci dss' in doc['name'].lower() or 'saq a' in doc['name'].lower()]
            
            if critical_docs:
                
                # Lance un t√©l√©chargement complet mais limit√©
                fallback_downloader = PCIScraperEnhanced(download_dir=download_dir)
                fallback_downloader.run()
                
                downloaded_files = [f for f in os.listdir(download_dir) if f.endswith('.pdf')] if os.path.exists(download_dir) else []
                
                if downloaded_files:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement de fallback: {e}")
            return False
    
    def log_session_summary(self, changes: Dict[str, List] = None):
        """
        Affiche un r√©sum√© de session dans les logs (sans cr√©er de fichier)
        
        Args:
            changes: Dictionnaire des changements d√©tect√©s (optionnel)
        """
        logger.info("R√âSUM√â DE SESSION:")
        logger.info(f"   Documents v√©rifi√©s: {self.stats['documents_checked']}")
        logger.info(f"   Changements d√©tect√©s: {self.stats['changes_detected']}")
        logger.info(f"   T√©l√©chargements r√©ussis: {self.stats['downloads_successful']}")
        logger.info(f"   Fichiers extraits (CSV): {self.stats['extracted_files']}")
        
        if changes and self.stats['changes_detected'] > 0:
            logger.info("D√âTAIL DES CHANGEMENTS:")
            logger.info(f"   ‚Ä¢ Nouveaux documents: {len(changes['new_documents'])}")
            logger.info(f"   ‚Ä¢ Versions mises √† jour: {len(changes['updated_versions'])}")
            logger.info(f"   ‚Ä¢ Documents supprim√©s: {len(changes['removed_documents'])}")
        
        if self.extracted_csv_files:
            logger.info("FICHIERS CSV G√âN√âR√âS:")
            for csv_file in self.extracted_csv_files:
                logger.info(f"   üìÑ {os.path.basename(csv_file)}")
        
        status = "SUCC√àS" if self.stats['changes_detected'] == 0 or self.stats['downloads_successful'] > 0 else "‚ö†Ô∏è PARTIEL"
        logger.info(f"STATUT FINAL: {status}")
    
    def send_email_summary(self, changes: Dict[str, List] = None, execution_time: float = 0):
        """
        Envoie un email r√©capitulatif de la session de scraping
        
        Args:
            changes: Dictionnaire des changements d√©tect√©s (optionnel)
            execution_time: Temps d'ex√©cution en secondes
        """
        try:
            logger.info("üìß Envoi du r√©capitulatif par email...")
            
            # D√©termine le statut
            if self.stats['changes_detected'] == 0:
                status = "Aucun Changement"
                status_class = "status-success"
            elif self.stats['downloads_successful'] > 0:
                status = "Succ√®s"
                status_class = "status-success"
            else:
                status = "Probl√®me"
                status_class = "status-warning"
            
            # G√©n√®re le contenu HTML
            html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <style>
        * {{ font-family: "Geist", sans-serif; font-weight: 100; }}
        html, body {{ margin: 0; padding: 0; height: 100%; background-color: #f5f5f5; }}
        body {{ color: white; }}
        .container {{ max-width: 600px; margin: 0 auto; padding: 20px; }}
        .main-title {{ font-size: 96px; line-height: 0.8; font-weight: 100; color: black; text-align: left; margin-bottom: 30px; }}
        .card {{ background: black; padding: 20px; border-radius: 12px; margin: 10px 0; position: relative; }}
        .title {{ font-size: 28px; margin-bottom: 8px; font-weight: 100; }}
        .subtitle {{ font-size: 14px; color: #ccc; margin-bottom: 20px; font-weight: 100; }}
        .separator {{ height: 1px; background: #333; margin: 20px 0; }}
        .section-title {{ font-size: 18px; margin-bottom: 15px; font-weight: 100; }}
        .stats-grid {{ display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 15px; }}
        .stat-item {{ text-align: center; }}
        .stat-value {{ font-size: 20px; font-weight: 100; }}
        .stat-label {{ font-size: 14px; color: #ccc; font-weight: 100; }}
        .changes-section {{ margin-top: 20px; }}
        .doc-list {{ margin: 10px 0; }}
        .doc-item {{ padding: 8px 0; border-bottom: 1px solid #333; font-weight: 100; }}
        .doc-item:last-child {{ border-bottom: none; }}
        .status-indicator {{ position: absolute; top: 20px; right: 20px; display: flex; align-items: center; gap: 8px; }}
        .status-dot {{ width: 8px; height: 8px; border-radius: 50%; }}
        .status-text {{ font-size: 12px; color: #ccc; font-weight: 100; }}
        .status-success {{ background-color: #28a745; }}
        .status-warning {{ background-color: #ffc107; }}
        .status-error {{ background-color: #dc3545; }}
        strong {{ font-weight: 100; }}
        .csv-link {{ color: white; text-decoration: none; font-weight: 100; transition: all 0.2s ease; }}
        .csv-link:hover {{ text-decoration: underline; color: #f0f0f0; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="main-title">PCI-DSS<br>Scanner</div>
        <div class="card">
            <div class="status-indicator">
                <div class="status-dot {status_class}"></div>
                <div class="status-text">Status</div>
            </div>
            
            <div class="title">Session Statistics</div>
            <div class="subtitle">Scan on {datetime.now().strftime('%d/%m/%Y at %I:%M:%S %p')}</div>
            
            <div class="separator"></div>
            
            <div class="section-title">Changes Details</div>
            <div class="stats-grid">
                <div class="stat-item">
                    <div class="stat-value">{len(changes['new_documents']) if changes else 0}</div>
                    <div class="stat-label">New documents</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{len(changes['updated_versions']) if changes else 0}</div>
                    <div class="stat-label">Updated versions</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{self.stats['downloads_successful']}</div>
                    <div class="stat-label">Successful downloads</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{self.stats['extracted_files']}</div>
                    <div class="stat-label">Extracted CSV files</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{execution_time:.2f} seconds</div>
                    <div class="stat-label">Execution time</div>
                </div>
            </div>
            
            <div class="separator"></div>"""
            
            # Ajoute les d√©tails des changements si il y en a
            if changes and self.stats['changes_detected'] > 0:
                html_content += """
            <div class="changes-section">"""
                
                # Liste les nouveaux documents
                if changes['new_documents']:
                    html_content += '<div style="margin-top: 20px;"><strong>New documents:</strong></div><div class="doc-list">'
                    for doc in changes['new_documents']:
                        html_content += f'<div class="doc-item">{doc["name"]} ({doc["category"]})</div>'
                    html_content += "</div>"
                
                # Liste les documents mis √† jour
                if changes['updated_versions']:
                    html_content += '<div class="section-title" style="margin-top: 20px;"><strong>Updated documents:</strong></div><div class="doc-list">'
                    for change in changes['updated_versions']:
                        html_content += f'<div class="doc-item">{change["name"]} ({change["category"]}) - {change["old_version"]} ‚Üí {change["new_version"]}</div>'
                    html_content += "</div>"
                
                html_content += "</div>"
            
            # Ajoute la section des fichiers CSV extraits
            if self.extracted_csv_files:
                html_content += """
            <div class="changes-section">
                <div class="section-title" style="margin-top: 20px;"><strong>CSV Files Generated:</strong></div>
                <div class="doc-list">"""
                
                for csv_file in self.extracted_csv_files:
                    filename = os.path.basename(csv_file)
                    
                    # D√©termine la langue bas√©e sur le nom de fichier pour l'affichage
                    language = self.detect_document_language(filename)
                    
                    # Mapping des langues vers drapeaux et labels
                    language_display = {
                        'EN': ("üá¨üáß", "EN"),
                        'FR': ("üá´üá∑", "FR"),
                        'ES': ("üá™üá∏", "ES"),
                        'DE': ("üá©üá™", "DE"),
                        'PT': ("üáµüáπ", "PT")
                    }
                    
                    flag_emoji, lang_label = language_display.get(language, ("üá¨üáß", "EN"))
                    
                    try:
                        # Lire le contenu CSV pour cr√©er un lien de t√©l√©chargement direct
                        with open(csv_file, 'r', encoding='utf-8') as f:
                            csv_content = f.read()
                        
                        # Cr√©er un data URL pour t√©l√©chargement direct
                        import urllib.parse
                        csv_encoded = urllib.parse.quote(csv_content)
                        data_url = f"data:text/csv;charset=utf-8,{csv_encoded}"
                        
                        html_content += f'<div class="doc-item"><a href="{data_url}" download="{filename}" class="csv-link">{flag_emoji} {filename}</a> <span style="color: #ccc; font-size: 12px;">({lang_label} - click to download)</span></div>'
                    except Exception as e:
                        # Fallback si erreur de lecture
                        html_content += f'<div class="doc-item"><span class="csv-link">{flag_emoji} {filename}</span> <span style="color: #ccc; font-size: 12px;">({lang_label} - see attachments)</span></div>'
                
                html_content += """
                </div>
            </div>"""
            
            html_content += f"""
        </div>
    </div>
</body>
</html>"""
            
            # D√©termine le sujet selon le statut
            if self.stats['changes_detected'] > 0:
                subject = f"PCI Scraper: {self.stats['changes_detected']} changement(s) d√©tect√©(s)"
            else:
                subject = "PCI Scraper: Aucun changement d√©tect√©"
            
            # Pr√©pare l'email avec pi√®ces jointes si des fichiers JSON sont disponibles
            email_data = {
                "from": "onboarding@resend.dev",
                "to": os.getenv('EMAIL_RECIPIENT', "mionnet.thom@gmail.com"), 
                "subject": subject,
                "html": html_content
            }
            
            # Ajoute les pi√®ces jointes CSV
            if self.extracted_csv_files:
                attachments = []
                for csv_file_path in self.extracted_csv_files:
                    try:
                        with open(csv_file_path, 'r', encoding='utf-8') as f:
                            csv_content = f.read()
                        
                        # Encode en base64 pour l'envoi
                        csv_base64 = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')
                        
                        filename = os.path.basename(csv_file_path)
                        attachment = {
                            "filename": filename,
                            "content": csv_base64
                        }
                        attachments.append(attachment)
                        logger.info(f"üìé Pi√®ce jointe ajout√©e: {filename}")
                        
                    except Exception as e:
                        logger.warning(f"Erreur lors de l'ajout de la pi√®ce jointe {csv_file_path}: {e}")
                
                if attachments:
                    email_data["attachments"] = attachments
                    logger.info(f"üìß Email avec {len(attachments)} pi√®ce(s) jointe(s)")
            
            # Envoie l'email
            response = resend.Emails.send(email_data)
            
            logger.info("Email r√©capitulatif envoy√© avec succ√®s")
            logger.info(f"ID de l'email: {response.get('id', 'N/A')}")
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'envoi de l'email: {e}")
            return False

    def run(self) -> bool:
        """
        Ex√©cute le processus complet : d√©tection + t√©l√©chargement automatique
        
        Returns:
            True si tout s'est bien pass√©, False sinon
        """
        start_time = time.time()
        success = False
        changes = None
        
        try:
            logger.info("D√©marrage du PCI Auto Scraper")
            logger.info(f"Session: {self.timestamp}")
            
            # Cr√©e le r√©pertoire de t√©l√©chargement
            os.makedirs(self.download_dir, exist_ok=True)
            
            # Configure les scrapers
            if not self.setup_scrapers():
                return False
            
            # √âtape 1: D√©tection des changements
            logger.info("\n" + "="*50)
            logger.info("√âTAPE 1: D√âTECTION DES CHANGEMENTS")
            logger.info("="*50)
            
            changes = self.detect_changes()
            if changes is None:
                logger.error("√âchec de la d√©tection de changements")
                return False
            
            # √âtape 2: D√©cision de t√©l√©chargement
            logger.info("\n" + "="*50)
            logger.info("√âTAPE 2: ANALYSE DES CHANGEMENTS")
            logger.info("="*50)
            
            should_download = self.should_download(changes)
            
            # √âtape 3: T√©l√©chargement si n√©cessaire
            if should_download:
                logger.info("\n" + "="*50)
                logger.info("√âTAPE 3: T√âL√âCHARGEMENT DES DOCUMENTS")
                logger.info("="*50)
                
                download_success = self.download_changed_documents(changes)
                if download_success:
                    logger.info("T√©l√©chargement termin√© avec succ√®s")
                    success = True
                else:
                    logger.warning("T√©l√©chargement partiellement r√©ussi ou √©chou√©")
            else:
                logger.info("Aucun t√©l√©chargement n√©cessaire - tous les documents sont √† jour")
                success = True
            
            return success
            
        except Exception as e:
            logger.error(f"Erreur dans le processus principal: {e}")
            return False
        
        finally:
            # Affiche le r√©sum√© final
            execution_time = time.time() - start_time
            logger.info(f"\nTemps d'ex√©cution: {execution_time:.2f} secondes")
            
            self.log_session_summary(changes)
            
            # Envoi du r√©capitulatif par email
            self.send_email_summary(changes, execution_time)
            
            # Nettoyage
            if self.change_detector and self.change_detector.driver:
                self.change_detector.close()
            
            # R√©sum√© final
            logger.info("\n" + "="*50)
            logger.info("R√âSUM√â DE LA SESSION")
            logger.info("="*50)
            logger.info(f"Documents v√©rifi√©s: {self.stats['documents_checked']}")
            logger.info(f"Changements d√©tect√©s: {self.stats['changes_detected']}")
            logger.info(f"T√©l√©chargements r√©ussis: {self.stats['downloads_successful']}")
            logger.info(f"Dur√©e: {execution_time:.2f}s")
            logger.info(f"Statut: {'SUCC√àS' if success else '√âCHEC'}")

def main():
    """Fonction principale - Lance automatiquement la d√©tection et le t√©l√©chargement"""
    try:
        print("D√©marrage du PCI Auto Scraper")
        print("D√©tection automatique des changements et t√©l√©chargement des documents PCI DSS/SAQ")
        print("=" * 80)
        
        # Cr√©e et lance le scraper automatique en mode headless par d√©faut
        auto_scraper = PCIAutoScraper(
            headless=True,  # Mode headless pour fonctionnement automatique
            download_dir='downloads'
        )
        
        success = auto_scraper.run()
        
        if success:
            print("\nPCI Auto Scraper termin√© avec succ√®s !")
            print("V√©rifiez le dossier 'downloads' pour les nouveaux fichiers")
            sys.exit(0)
        else:
            print("\nPCI Auto Scraper termin√© avec des erreurs")
            print("Consultez les logs ci-dessus pour plus de d√©tails")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nArr√™t demand√© par l'utilisateur")
        sys.exit(130)
    except Exception as e:
        logger.error(f"Erreur fatale: {e}")
        print(f"\nErreur fatale: {e}")
        print("V√©rifiez que les d√©pendances sont install√©es: pip install -r requirements.txt")
        sys.exit(1)

if __name__ == "__main__":
    main()
